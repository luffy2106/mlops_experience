{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune Mistral models using ü§ó [`peft`](https://github.com/huggingface/peft) adapters, [`transformers`](https://github.com/huggingface/transformers) & [`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes)\n",
    "\n",
    "\n",
    "This notebook is made by following this tutorial :\n",
    "\n",
    "https://github.com/huggingface/notebooks/blob/main/sagemaker/28_train_llms_with_qlora/sagemaker-notebook.ipynb\n",
    "\n",
    "https://github.com/philschmid/deep-learning-pytorch-huggingface/blob/main/training/fine-tune-llms-in-2024-with-trl.ipynb\n",
    "\n",
    "To understand about how Lora work, take a look at this link:\n",
    "\n",
    "https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
    "\n",
    "https://blog.dailydoseofds.com/p/full-model-fine-tuning-vs-lora-vs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-Introduction\n",
    "\n",
    "In this sagemaker example, we are going to learn how to apply [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
    "to fine-tune Mistral LLM. QLoRA is an efficient finetuning technique that quantizes a pretrained language model to 4 bits and attaches small ‚ÄúLow-Rank Adapters‚Äù which are fine-tuned. This enables fine-tuning of models with up to 65 billion parameters on a single GPU; despite its efficiency, QLoRA matches the performance of full-precision fine-tuning and achieves state-of-the-art results on language tasks.\n",
    "\n",
    "In our example, we are going to leverage Hugging Face [Transformers](https://huggingface.co/docs/transformers/index), [Accelerate](https://huggingface.co/docs/accelerate/index), and [PEFT](https://github.com/huggingface/peft).\n",
    "\n",
    "In Detail you will learn how to:\n",
    "1. Setup Development Environment\n",
    "2. Load and prepare the dataset\n",
    "3. Fine-Tune Mistral LLM with QLoRA on Amazon SageMaker\n",
    "\n",
    "### Quick intro: PEFT or Parameter Efficient Fine-tuning\n",
    "\n",
    "[PEFT](https://github.com/huggingface/peft), or Parameter Efficient Fine-tuning, is a new open-source library from Hugging Face to enable efficient adaptation of pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model's parameters. PEFT currently includes techniques for:\n",
    "\n",
    "- (Q)LoRA:¬†[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "- Prefix Tuning:¬†[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf)\n",
    "- P-Tuning:¬†[GPT Understands, Too](https://arxiv.org/pdf/2103.10385.pdf)\n",
    "- Prompt Tuning:¬†[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Environment Setup\n",
    "\n",
    "\n",
    "Before you begin, make sure you have the necessary environment and dependencies set up. This notebook was tested in Amazon SageMaker Studio with the following configuration:\n",
    "\n",
    "- **Python Version:** Pytorch 2.1.0 Python 3.10 kernel\n",
    "- **Instance Type:** ml.g5.12xlarge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- %pip install -q -U  bitsandbytes==0.41.3\n",
    "%pip install -q -U transformers==4.36.2 peft==0.7.1 accelerate==0.25.0 trl==0.7.7\n",
    "%pip install -q -U datasets==2.16.0  wandb==0.16.1 sentencepiece==0.1.99 openpyxl\n",
    "%pip install trl==0.7.7 -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-experience-PE_IGGo5-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
